{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 5;\n",
    "goal_state = (grid_size - 1,grid_size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obstackle\n",
    "obstackles = [(1,2),(3,1),(2,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize q table with zeroes(gridsize * gridsize * number of actions)\n",
    "# Initialize Q-table with zeros: (grid_size x grid_size x num_actions)\n",
    "# We have two possible actions (right, down), so last dimension is 2\n",
    "Q = np.zeros((grid_size,grid_size,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "alpha = 0.8\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [(0,1),(1,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state):\n",
    "    if state == goal_state:\n",
    "        return 100;\n",
    "    elif state in obstackles:\n",
    "        return -10;\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_move(state,action):\n",
    "    new_state = (state[0] + action[0] , state[1]+action[1])\n",
    "\n",
    "    if 0 <= new_state[0] < grid_size and 0<=new_state[1] < grid_size:\n",
    "        if new_state not in obstackles:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state,action):\n",
    "    return (state[0] + action[0] , state[1]+action[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q learning algorithms\n",
    "\n",
    "for episode in range(5):\n",
    "    state = (0,0);\n",
    "\n",
    "    while state != goal_state:\n",
    "        # choose action using epsilon greedy technique strategy\n",
    "        if epsilon > random.uniform(0,1):\n",
    "            action_index = random.randint(0,1) #explore ... by choosing random action 0 or 1\n",
    "        else:\n",
    "            # exploit -- choose action with highest Q-value in the current state \n",
    "            action_index = np.argmax(Q[state[0],state[1], :])\n",
    "        \n",
    "        action = actions[action_index]\n",
    "\n",
    "\n",
    "        if not is_valid_move(state, action):\n",
    "            continue #skip to next move if the move is invalid\n",
    "\n",
    "\n",
    "        next_state = get_next_state(state,action)\n",
    "        reward = get_reward(next_state)\n",
    "\n",
    "        # q-learning formula : \n",
    "        # update q value for the (state,action) pair ... caculating best next action for the next state because we need itin bellman equation\n",
    "        best_next_action = np.argmax(Q[next_state[0],next_state[1], :])\n",
    "\n",
    "        Q[state[0],state[1],action_index] = Q[state[0], state[1], action_index] + alpha*(reward + gamma * Q[next_state[0],next_state[1],best_next_action] - Q[state[0], state[1], action_index] )\n",
    " \n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ -1.89827236  42.56348809]\n",
      "  [ -0.99968     38.40075677]\n",
      "  [ -1.82912      0.        ]\n",
      "  [ -0.992       -0.96      ]\n",
      "  [  0.          -0.992     ]]\n",
      "\n",
      " [[ 48.45390411  -1.568     ]\n",
      "  [  0.          54.95357815]\n",
      "  [  0.           0.        ]\n",
      "  [ -0.8         49.20638464]\n",
      "  [  0.           0.        ]]\n",
      "\n",
      " [[ -0.8         -0.96      ]\n",
      "  [ 62.17097568   0.        ]\n",
      "  [ 70.18999863  -1.568     ]\n",
      "  [  0.          79.09999994]\n",
      "  [  0.           0.        ]]\n",
      "\n",
      " [[  0.          -0.96      ]\n",
      "  [  0.           0.        ]\n",
      "  [ 56.38656     -0.8       ]\n",
      "  [ 89.          -0.8       ]\n",
      "  [  0.         100.        ]]\n",
      "\n",
      " [[ 27.90912      0.        ]\n",
      "  [ 64.5888       0.        ]\n",
      "  [ 86.56         0.        ]\n",
      "  [ 99.84         0.        ]\n",
      "  [  0.           0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_optimal_path(Q):\n",
    "    state = (0, 0)  # Start at the top-left corner\n",
    "    path = [state]  # List to store the path\n",
    "\n",
    "    # Traverse until the goal state is reached\n",
    "    while state != goal_state:\n",
    "        # Choose action with the highest Q-value (exploit)\n",
    "        action_index = np.argmax(Q[state[0], state[1], :])\n",
    "        action = actions[action_index]\n",
    "\n",
    "        # Get next state\n",
    "        next_state = get_next_state(state, action)\n",
    "        \n",
    "        # Add the next state to the path\n",
    "        path.append(next_state)\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Path:\n",
      "[(0, 0), (1, 0), (1, 1), (2, 1), (2, 2), (2, 3), (3, 3), (3, 4), (4, 4)]\n",
      "Robot is at (0, 0)\n",
      "Robot is at (1, 0)\n",
      "Robot is at (1, 1)\n",
      "Robot is at (2, 1)\n",
      "Robot is at (2, 2)\n",
      "Robot is at (2, 3)\n",
      "Robot is at (3, 3)\n",
      "Robot is at (3, 4)\n",
      "Robot is at (4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Extract the optimal path\n",
    "optimal_path = extract_optimal_path(Q)\n",
    "\n",
    "# Print the optimal path\n",
    "print(\"Optimal Path:\")\n",
    "print(optimal_path)\n",
    "\n",
    "# Simulate the robot's movement on the grid\n",
    "for state in optimal_path:\n",
    "    print(f\"Robot is at {state}\")\n",
    "\n",
    "# [(1,2),(3,1),(2,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('optimal_path.txt','w' ) as file:\n",
    "    for state in optimal_path:\n",
    "        line = '\\t'.join(str(i) for i in state)\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
